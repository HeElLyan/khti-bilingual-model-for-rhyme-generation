{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mixed_cross_rus_roberta.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNawbjZDdPhrZMAMpQQ8vNZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBQ9K9mflkT1","executionInfo":{"status":"ok","timestamp":1652007417260,"user_tz":-180,"elapsed":2429,"user":{"displayName":"He El","userId":"11033579805504794623"}},"outputId":"81b84215-9d51-46b1-f91d-74eb3c07aad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('gdrive')"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"SE8pkhFQOkUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaTokenizer, RobertaForMaskedLM\n","import torch\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"5_VyXjBoOldA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'sberbank-ai/ruRoberta-large'"],"metadata":{"id":"N0EAFhezOmp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = RobertaForMaskedLM.from_pretrained(model_name)"],"metadata":{"id":"VcZlw5UrOqRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEFAULT_FOLDER = 'gdrive/My Drive/diploma/'\n","RULE = 'cross'\n","LANGUAGE = 'rus'\n","data_names = ['train', 'test', 'val']\n","genres = ['rock', 'rap', 'pop']\n","full_lines = True"],"metadata":{"id":"kVmsPm_3QaDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define paths\n","if full_lines:\n","    data_paths = [DEFAULT_FOLDER + 'dataset_full_lines/' + genre + '/' + name + '_' + RULE + '_' + LANGUAGE + '.txt' for genre in genres for name in data_names]\n","else:\n","    data_paths = [DEFAULT_FOLDER + 'dataset/' + genre + '/' + name + '_' + RULE + '_' + LANGUAGE + '.txt' for genre in genres for name in data_names]\n","data_paths"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUTavKOpQcJC","executionInfo":{"status":"ok","timestamp":1652007452703,"user_tz":-180,"elapsed":25,"user":{"displayName":"He El","userId":"11033579805504794623"}},"outputId":"b0f6d332-82b0-4c2f-cb67-5654ad7e3b0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['gdrive/My Drive/diploma/dataset/rock/train_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/rock/test_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/rock/val_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/rap/train_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/rap/test_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/rap/val_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/pop/train_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/pop/test_cross_rus.txt',\n"," 'gdrive/My Drive/diploma/dataset/pop/val_cross_rus.txt']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import pickle\n","\n","def get_data(paths):\n","\n","    data = []\n","\n","    loc_list = []\n","\n","    #load data\n","    for i, data_path in enumerate(paths):\n","\n","        with open(data_path, 'rb') as fp:\n","            data_single = pickle.load(fp)\n","\n","        loc_list.append(data_single)\n","\n","        if (i + 1) % len(data_names) == 0:\n","            data.append(loc_list)\n","            loc_list = []\n","            \n","    return data\n","\n","def get_data_shape(data):\n","\n","    shape_list = []\n","\n","    for data_genre in data:\n","\n","        loc_list = []\n","        for data_type in data_genre:\n","            loc_list.append(len(data_type))\n","\n","        shape_list.append(loc_list)\n","        \n","    return shape_list\n","\n","def get_max_text_genres_len(genres):\n","    genres_name_len = [len(genre) for genre in genres]\n","    TEXT_GENRES_LEN = max(genres_name_len)\n","    return TEXT_GENRES_LEN\n","\n","TEXT_GENRES_LEN = get_max_text_genres_len(genres)\n","\n","def format_data(data):\n","\n","    shape_list = get_data_shape(data)\n","\n","    for i, data in enumerate(shape_list):\n","        print('{} ({} {} {})'.format(' ' * (TEXT_GENRES_LEN - len(genres[i])) + genres[i], data[0], data[1], data[2]))"],"metadata":{"id":"yTJ20AxBQewO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_genres = get_data(data_paths)\n","format_data(data_genres)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jlYIwjsQk2h","executionInfo":{"status":"ok","timestamp":1652007452704,"user_tz":-180,"elapsed":20,"user":{"displayName":"He El","userId":"11033579805504794623"}},"outputId":"57befc07-e9ef-4992-a28c-b55c9c2a1c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rock (10288 1286 1286)\n"," rap (24939 3117 3118)\n"," pop (5833 729 730)\n"]}]},{"cell_type":"code","source":["TRAIN = 5000\n","TEST, VALID = 500, 500\n","types_num = [TRAIN, TEST, VALID]"],"metadata":{"id":"D-Xsym0JQ0TO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def get_random_data(nums, data):\n","\n","    updated_data = []\n","\n","    for data_genre in data:\n","\n","        loc_list = []\n","        for i, data_type in enumerate(data_genre): \n","            data_single = random.choices(data_type, k=nums[i])\n","            loc_list.append(data_single)\n","        \n","        updated_data.append(loc_list)\n","        \n","    return updated_data\n","\n","def concat_data(data):\n","\n","    updated_data = []\n","\n","    for data_genre in data:\n","\n","        updated_data += data_genre\n","    \n","    concat_data = []\n","\n","    for i in range(len(types_num)):\n","\n","        loc_list = []\n","        for j in range(i, len(updated_data), 3):\n","            # print(updated_data[j])\n","            # print(j)\n","            loc_list += updated_data[j]\n","\n","        # print(len(loc_list))\n","\n","        concat_data.append(loc_list)\n","\n","\n","    return concat_data"],"metadata":{"id":"eXHWPiqPQ2aE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","\n","#find max tokens len\n","def get_max_tokens_len(data):\n","\n","    token_lens = []\n","\n","    for text in data:\n","        loc_tokens = tokenizer.encode(text, max_length=512, truncation=True)\n","        token_lens.append(len(loc_tokens))\n","\n","    # sns.distplot(token_lens)\n","    max_length = max(token_lens)\n","\n","    # print(max_length)\n","    return max_length\n","\n","def get_max_len(data):\n","\n","    max_tokens_len_list = []\n","\n","    for item in data:\n","        max_tokens_len = get_max_tokens_len(item)\n","        max_tokens_len_list.append(max_tokens_len)\n","    return max(max_tokens_len_list)"],"metadata":{"id":"McWW40FTQ5J3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 1000\n","MIN_LEN = 85\n","\n","if full_lines:\n","    MIN_LEN = MIN_LEN * 2\n","\n","while MAX_LEN > MIN_LEN:\n","    data_genres_updated = get_random_data(types_num, data_genres)\n","    # format_data(data_genres_updated)\n","    data_genres_concat = concat_data(data_genres_updated)\n","    MAX_LEN = get_max_len(data_genres_concat)\n","    # print(MAX_LEN)\n","\n","MAX_LEN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlqbYt9zQ706","executionInfo":{"status":"ok","timestamp":1652007461179,"user_tz":-180,"elapsed":8084,"user":{"displayName":"He El","userId":"11033579805504794623"}},"outputId":"d8476a83-ef3e-4767-c229-c37eaf43eeea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["class SongsTextsDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, songs, tokenizer, max_len):\n","    self.songs = songs\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.songs)\n","  \n","  def __getitem__(self, item):\n","    song = str(self.songs[item])\n","\n","    encoding = self.tokenizer.encode_plus(\n","      song,\n","      max_length=self.max_len,\n","      truncation=True,\n","      padding='max_length',\n","      return_tensors='pt',\n","    )\n","\n","    encoding['labels'] = encoding.input_ids.detach().clone()\n","\n","    mask_arr = (encoding.input_ids != 0) * (encoding.input_ids != 1) * (encoding.input_ids != 2) * (encoding.input_ids != 3)\n","    \n","    selection = []\n","\n","    for i in range(encoding.input_ids.shape[0]):\n","\n","      # get indices of mask positions from mask array\n","      res = mask_arr[i].nonzero()[-1]\n","\n","      # append mask position\n","      selection.append(torch.flatten(res).tolist())\n","\n","    for i in range(encoding.input_ids.shape[0]):\n","\n","      # mask input_ids\n","      encoding.input_ids[i, selection[i]] = 4\n","\n","    return {\n","      'song_text': song,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'labels': encoding['labels'].flatten(),\n","    }"],"metadata":{"id":"j60DzzH8RNbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_data_loader(songs, tokenizer, max_len, batch_size):\n","\n","  ds = SongsTextsDataset(\n","    songs=songs,\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return torch.utils.data.DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=2\n","  )"],"metadata":{"id":"EDHu0A6mRPal"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 16"],"metadata":{"id":"euGOP8SaRRbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_loader = create_data_loader(data_genres_concat[0], tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(data_genres_concat[1], tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(data_genres_concat[2], tokenizer, MAX_LEN, BATCH_SIZE)"],"metadata":{"id":"Juo5wZ24RTtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup GPU/CPU usage and activate the training mode of our model.\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# and move our model over to the selected device\n","model.to(device)"],"metadata":{"id":"07IRixFoRU-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 4"],"metadata":{"id":"hSu2wsVURW61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import torch.nn as nn\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LP4UzQt0RYfz","executionInfo":{"status":"ok","timestamp":1652007001167,"user_tz":-180,"elapsed":487,"user":{"displayName":"He El","userId":"11033579805504794623"}},"outputId":"3178d572-1db7-4b8d-8943-151868409880"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n","\n","    model = model.train()\n","\n","    losses = []\n","    corr_pred = 0\n","    \n","    for d in data_loader:\n","      \n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask,\n","          labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=-1)\n","\n","        for i in range(len(labels)):\n","            if tokenizer.decode(labels[i]) == tokenizer.decode(preds[i]):\n","                corr_pred += 1\n","            # print(tokenizer.decode(labels[i]))\n","            # print(tokenizer.decode(preds[i]))\n","\n","        loss = outputs.loss\n","\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return corr_pred / n_examples, np.mean(losses)"],"metadata":{"id":"EMSRSECCRZ8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def eval_model(model, data_loader, device, n_examples):\n","\n","    model = model.eval()\n","\n","    losses = []\n","    corr_pred = 0\n","\n","    with torch.no_grad():\n","\n","        for d in data_loader:\n","\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","        \n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=-1)\n","\n","            for i in range(len(labels)):\n","                if tokenizer.decode(labels[i]) == tokenizer.decode(preds[i]):\n","                    corr_pred += 1\n","                # print(mixed_tokenizer.decode(labels[i]))\n","                # print(mixed_tokenizer.decode(preds[i]))\n","\n","            loss = outputs.loss\n","\n","            losses.append(loss.item())\n","    \n","    return corr_pred / n_examples, np.mean(losses)"],"metadata":{"id":"HRBU_LSNRb1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os.path\n","from os import path\n","\n","def create_folder(folder_name):\n","    if path.exists(folder_name) == False:\n","        os.mkdir(folder_name)"],"metadata":{"id":"oEzhS3QhRc9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if full_lines:\n","    if len(genres) > 1:\n","        model_save_path = 'gdrive/My Drive/diploma/models/mixed_cross_roberta_batch' + str(BATCH_SIZE) + '_' + LANGUAGE + '_full_lines/'\n","    else:\n","        model_save_path = 'gdrive/My Drive/diploma/models/' + genres[0] + '_cross_roberta_batch' + str(BATCH_SIZE) + '_' + LANGUAGE + '_full_lines/'\n","else:\n","    if len(genres) > 1:\n","        model_save_path = 'gdrive/My Drive/diploma/models/mixed_cross_roberta_batch' + str(BATCH_SIZE) + '_' + LANGUAGE + '/'\n","    else:\n","        model_save_path = 'gdrive/My Drive/diploma/models/' + genres[0] + '_cross_roberta_batch' + str(BATCH_SIZE) + '_' + LANGUAGE + '/'\n","    \n","create_folder(model_save_path)"],"metadata":{"id":"c6_bsDjnReSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","from collections import defaultdict\n","\n","history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,    \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(data_genres_concat[0])\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    device, \n","    len(data_genres_concat[2])\n","  )\n","\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","\n","  if val_acc > best_accuracy:\n","    model.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","    best_accuracy = val_acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLz_hlTSR0y6","outputId":"9d17e1b8-2564-49eb-f870-75b485272ab5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","----------\n","Train loss 3.5489754498004915 accuracy 0.0\n","Val   loss 0.24466693103313447 accuracy 0.0\n","\n","Epoch 2/4\n","----------\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])"],"metadata":{"id":"B9BgU9voR2K7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_acc, _ = eval_model(\n","  model,\n","  test_data_loader,\n","  device,\n","  len(data_genres_concat[1])\n",")\n","\n","test_acc"],"metadata":{"id":"S6jIkLTbR3H0"},"execution_count":null,"outputs":[]}]}