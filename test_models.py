# -*- coding: utf-8 -*-
"""mixed_cross_test_all.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jDYlE6C33AV5zGADIsU1cC8aiHRtk_FB
"""

from google.colab import drive
drive.mount('gdrive')

!pip install transformers

DEFAULT_FOLDER = 'gdrive/My Drive/diploma/'
RULE = 'cross'
LANGUAGE = 'eng'
data_names = ['test']
genres = ['rock', 'rap', 'pop', 'metal']

#define paths
data_paths = [DEFAULT_FOLDER + 'dataset/' + genre + '/' + name + '_' + RULE + '_' + LANGUAGE + '.txt' for genre in genres for name in data_names]
data_paths

import pickle

def get_data(paths):

    data = []

    # loc_list = []

    #load data
    for i, data_path in enumerate(paths):

        with open(data_path, 'rb') as fp:
            data_single = pickle.load(fp)

        data.append(data_single)

        # if (i + 1) % len(data_names) == 0:
        #     data.append(loc_list)
        #     loc_list = []
            
    return data

def get_data_shape(data):

    shape_list = []

    for data_genre in data:

        shape_list.append(len(data_genre))

    return shape_list

def get_max_text_len(data):
    names_len = [len(str(text)) for text in data]
    TEXT_LEN = max(names_len)
    return TEXT_LEN

TEXT_GENRES_LEN = get_max_text_len(genres)

def format_data(data, brackets=True):
    if brackets:
        for i, genre in enumerate(genres):
            print('{} ({})'.format(' ' * (TEXT_GENRES_LEN - len(genre)) + genre, data[i]))
    else:
        for i, genre in enumerate(genres):
            print('{} {}'.format(' ' * (TEXT_GENRES_LEN - len(genre)) + genre, data[i]))

def get_data_and_shape(paths):

    data = get_data(paths)
    shape = get_data_shape(data)
    # format_data(shape)

    return data

import random

def get_random_data(num, data):
    
    updated_data = []

    for data_genre in data:

        data_single = random.choices(data_genre, k=num)
        updated_data.append(data_single)
        
    return updated_data

def get_random_data_and_shape(num, paths):

    data = get_data_and_shape(paths)

    random_data = get_random_data(num, data)
    shape = get_data_shape(random_data)
    # format_data(shape)

    return random_data

import seaborn as sns

#find max tokens len
def get_max_tokens_len(data, tokenizer):

    token_lens = []

    for text in data:
        loc_tokens = tokenizer.encode(text, max_length=512, truncation=True)
        token_lens.append(len(loc_tokens))

    # sns.distplot(token_lens)
    MAX_LEN = max(token_lens)

    # print(MAX_LEN)
    return MAX_LEN

def get_max_len(data, tokenizer):

    max_tokens_len_list = []

    for item in data:
        max_tokens_len = get_max_tokens_len(item, tokenizer)
        max_tokens_len_list.append(max_tokens_len)
        
    return max(max_tokens_len_list)

import torch
torch.cuda.empty_cache()

from transformers import BertTokenizer, BertForMaskedLM, RobertaTokenizer, RobertaForMaskedLM

def load_model(modelName, BATCH_SIZE, LANGUAGE, fullLines, genre):

    model_save_path = 'gdrive/My Drive/diploma/models/' + genre + '_cross_' + modelName + '_batch' + str(BATCH_SIZE) + '_' + LANGUAGE 

    if not fullLines:
        model_path = model_save_path + '/'
    else:
        model_path = model_save_path + '_full_lines/'

    if modelName == 'bert':
        
        model = BertForMaskedLM.from_pretrained(model_path)
        tokenizer = BertTokenizer.from_pretrained(model_path)

    elif modelName == 'roberta':

        model = RobertaForMaskedLM.from_pretrained(model_path)
        tokenizer = RobertaTokenizer.from_pretrained(model_path)

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    model.to(device)

    return tokenizer, model, device

class SongsTextsDatasetBert(torch.utils.data.Dataset):

    def __init__(self, songs, tokenizer, MAX_LEN):
        self.songs = songs
        self.tokenizer = tokenizer
        self.MAX_LEN = MAX_LEN
  
    def __len__(self):
        return len(self.songs)
    
    def __getitem__(self, item):
        song = str(self.songs[item])

        encoding = self.tokenizer.encode_plus(
        song,
        max_length=self.MAX_LEN,
        truncation=True,
        padding='max_length',
        return_tensors='pt',
        )

        encoding['labels'] = encoding.input_ids.detach().clone()

        mask_arr = (encoding.input_ids != 0) * (encoding.input_ids != 101) * (encoding.input_ids != 102) 

        selection = []

        for i in range(encoding.input_ids.shape[0]):

            # get indices of mask positions from mask array
            res = mask_arr[i].nonzero()[-1]

            # append mask position
            selection.append(torch.flatten(res).tolist())

        for i in range(encoding.input_ids.shape[0]):

            # mask input_ids
            encoding.input_ids[i, selection[i]] = 103

        return {
        'song_text': song,
        'input_ids': encoding['input_ids'].flatten(),
        'attention_mask': encoding['attention_mask'].flatten(),
        'labels': encoding['labels'].flatten(),
        }

class SongsTextsDatasetRoberta(torch.utils.data.Dataset):

    def __init__(self, songs, tokenizer, MAX_LEN):
        self.songs = songs
        self.tokenizer = tokenizer
        self.MAX_LEN = MAX_LEN
  
    def __len__(self):
        return len(self.songs)
  
    def __getitem__(self, item):
        song = str(self.songs[item])

        encoding = self.tokenizer.encode_plus(
            song,
            max_length=self.MAX_LEN,
            truncation=True,
            padding='max_length',
            return_tensors='pt',
        )

        encoding['labels'] = encoding.input_ids.detach().clone()

        mask_arr = (encoding.input_ids != 0) * (encoding.input_ids != 1) * (encoding.input_ids != 2) * (encoding.input_ids != 3)

        selection = []

        for i in range(encoding.input_ids.shape[0]):

            # get indices of mask positions from mask array
            res = mask_arr[i].nonzero()[-1]

            # append mask position
            selection.append(torch.flatten(res).tolist())

        for i in range(encoding.input_ids.shape[0]):

            # mask input_ids
            encoding.input_ids[i, selection[i]] = 50264
            # encoding.input_ids[i, selection[i]] = 4

        return {
            'song_text': song,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['labels'].flatten(),
        }

class SongsTextsDatasetRuRoberta(torch.utils.data.Dataset):

    def __init__(self, songs, tokenizer, MAX_LEN):
        self.songs = songs
        self.tokenizer = tokenizer
        self.MAX_LEN = MAX_LEN
  
    def __len__(self):
        return len(self.songs)
  
    def __getitem__(self, item):
        song = str(self.songs[item])

        encoding = self.tokenizer.encode_plus(
            song,
            max_length=self.MAX_LEN,
            truncation=True,
            padding='max_length',
            return_tensors='pt',
        )

        encoding['labels'] = encoding.input_ids.detach().clone()

        mask_arr = (encoding.input_ids != 0) * (encoding.input_ids != 1) * (encoding.input_ids != 2) * (encoding.input_ids != 3)

        selection = []

        for i in range(encoding.input_ids.shape[0]):

            # get indices of mask positions from mask array
            res = mask_arr[i].nonzero()[-1]

            # append mask position
            selection.append(torch.flatten(res).tolist())

        for i in range(encoding.input_ids.shape[0]):

            # mask input_ids
            # encoding.input_ids[i, selection[i]] = 50264
            encoding.input_ids[i, selection[i]] = 4

        return {
            'song_text': song,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['labels'].flatten(),
        }

def create_data_loader(songs, tokenizer, MAX_LEN, BATCH_SIZE, modelName, LANGUAGE):

    if modelName == 'bert':
        ds = SongsTextsDatasetBert(songs=songs, tokenizer=tokenizer, MAX_LEN=MAX_LEN)

    elif modelName == 'roberta':
        if LANGUAGE == 'rus':
            ds = SongsTextsDatasetRuRoberta(songs=songs, tokenizer=tokenizer, MAX_LEN=MAX_LEN)
        else:
            ds = SongsTextsDatasetRoberta(songs=songs, tokenizer=tokenizer, MAX_LEN=MAX_LEN)

    # print('Data loader {} is created'.format(modelName))

    return torch.utils.data.DataLoader(ds, BATCH_SIZE, num_workers=2)

import numpy as np

def eval_model(tokenizer, model, data_loader, device, n_examples):

    model = model.eval()

    losses = []
    corr_pred = 0

    with torch.no_grad():

        for d in data_loader:

            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            labels = d["labels"].to(device)
        
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            _, preds = torch.max(outputs.logits, dim=-1)

            for i in range(len(labels)):
                if tokenizer.decode(labels[i]) == tokenizer.decode(preds[i]):
                    corr_pred += 1
                # print(tokenizer.decode(labels[i]))
                # print(tokenizer.decode(preds[i]))

            loss = outputs.loss

            losses.append(loss.item())
    
    return corr_pred / n_examples, np.mean(losses)
    # return (preds == labels).float().mean(), np.mean(losses)

def get_acc_all_genres(data, tokenizer, model, device, MAX_LEN, BATCH_SIZE, modelName, LANGUAGE):

    accs = []

    for i, singe_genre in enumerate(data):

        data_loader = create_data_loader(singe_genre, tokenizer, MAX_LEN, BATCH_SIZE, modelName, LANGUAGE)
        test_acc, _ = eval_model(tokenizer, model, data_loader, device, len(singe_genre))
        # print('Got {} accuracy'.format(i))
        accs.append(round(test_acc, 3))

    return accs

def get_lower_max_len(paths, tokenizer, MIN_LEN, TEST):

    #get tokens max len
    MAX_LEN = 1000

    while MAX_LEN > MIN_LEN:
        data = get_random_data_and_shape(TEST, paths)
        MAX_LEN = get_max_len(data, tokenizer)

    return MAX_LEN, data

def test_model(model_names, batch_sizes, MIN_LEN=85, TEST=4000, LANGUAGE='eng', fullLines=False, genre='mixed'):

    #define paths
    if not fullLines:
        paths = [DEFAULT_FOLDER + 'dataset/' + genre + '/' + name + '_' + RULE + '_' + LANGUAGE + '.txt' for genre in genres for name in data_names]
    else:
        paths = [DEFAULT_FOLDER + 'dataset_full_lines/' + genre + '/' + name + '_' + RULE + '_' + LANGUAGE + '.txt' for genre in genres for name in data_names]

    MAX_LEN, data = 0, []
    all_accs = []

    for i, modelName in enumerate(model_names):

        for BATCH_SIZE in batch_sizes:

            tokenizer, model, device = load_model(modelName, BATCH_SIZE=BATCH_SIZE, LANGUAGE=LANGUAGE, fullLines=fullLines, genre=genre)

            #the same data for all cases
            if i == 0:
                #get tokens max len and data
                MAX_LEN, data = get_lower_max_len(paths, tokenizer, MIN_LEN, TEST)

            #get all genres accs
            accs = get_acc_all_genres(data, tokenizer, model, device, MAX_LEN, BATCH_SIZE, modelName, LANGUAGE)

            all_accs.append(accs)

            print('Got accuracy for model name {}, batch size {}'.format(modelName, BATCH_SIZE))

    format_accs(transpose_list(all_accs))

batch_sizes = [32]
model_names = ['bert', 'roberta']

headers = [model_name + str(batch_size) for batch_size in batch_sizes for model_name in model_names]
TEXT_HEADER_LEN = get_max_text_len(headers)

def format_accs(data):
    print()
    print(' ' * (TEXT_GENRES_LEN), get_headers(headers))

    for i, genre in enumerate(genres):
        print(' ' * (TEXT_GENRES_LEN - len(genre)) + genre, get_line_data(data[i]))

def get_single_header(data):
    part = ' ' * int((int(TEXT_HEADER_LEN) - len(str(data))) / 2) 
    line = part + data + part
    return line

def get_headers(data):
    headers = []
    for header in data:
        loc_header = get_single_header(header)
        headers.append(loc_header)
    
    res = ' '.join([header for header in headers])
    return res

def get_line_data(data):

    TEXT_DATA_LEN = get_max_text_len(data)

    lines = []
    for text in data:

        part = ' ' * int((int(TEXT_HEADER_LEN) - len(str(text))) / 2) 
        line = part + str(text) + part

        if len(str(text)) < TEXT_DATA_LEN:
            part_num = ' ' * int((TEXT_DATA_LEN - len(str(text))))
            line += part_num
        lines.append(line)

    res = ''.join([line for line in lines])
    return res

def transpose_list(data):
    return list(map(list, zip(*data)))

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=False, genre='mixed')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True)

batch_sizes = [32]
model_names = ['bert', 'roberta']

headers = [model_name + str(batch_size) for batch_size in batch_sizes for model_name in model_names]
TEXT_HEADER_LEN = get_max_text_len(headers)

genres = genres[:3]
TEXT_GENRES_LEN = get_max_text_len(genres)

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='rus', fullLines=False)

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='rus', fullLines=True)

batch_sizes = [32]
model_names = ['roberta']

headers = [model_name + str(batch_size) for batch_size in batch_sizes for model_name in model_names]
TEXT_HEADER_LEN = get_max_text_len(headers)

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True, genre='rock')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True, genre='rap')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True, genre='pop')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True, genre='metal')

genres = genres[:3]
TEXT_GENRES_LEN = get_max_text_len(genres)

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='rus', fullLines=True, genre='rock')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='rus', fullLines=True, genre='rap')

test_model(model_names, batch_sizes, TEST=4000, LANGUAGE='eng', fullLines=True, genre='pop')

"""**Testing**"""

special_tokens_roberta = [0, 1, 2, 3, 50264]
special_tokens_bert = [0, 100, 101, 102, 103]

from torch.nn import functional as F
import re
import string

def predict_rhyme(text, modelName='bert', topOne=True, topNum=10, BATCH_SIZE=16, LANGUAGE='eng', fullLines=False, genre='mixed'):
    
    tokenizer, model, _ = load_model(modelName=modelName, BATCH_SIZE=BATCH_SIZE, LANGUAGE=LANGUAGE, fullLines=fullLines, genre=genre)

    splitted = text.split('\n')

    masked_sentences, not_masked_sentences = [], []

    for i, sentence in enumerate(splitted):

        loc_res = re.findall(r"[\w']+|[.,!?;]", sentence)

        if i == 2 or i == 3:

            if loc_res[-1] not in string.punctuation:
                loc_res[-1] = tokenizer.mask_token

            else:
                loc_res[-2] = tokenizer.mask_token
        
        loc_str = ''
        for word in loc_res:
            
            if word not in string.punctuation:
                loc_str += ' ' + word

            else:
                loc_str += word
        
        line = loc_str.strip() + '\n'

        if i == 2 or i == 3:
            masked_sentences.append(line)
        else:
            not_masked_sentences.append(line)

    masked_result, not_masked_result = [], []

    for i, sentence in enumerate(masked_sentences):

        sentence_toks = tokenizer.encode_plus(sentence, return_tensors = "pt")

        outputs = model(**sentence_toks)

        softmax = F.softmax(outputs.logits, dim = - 1)

        mask_indexes = torch.where(sentence_toks["input_ids"][0] == tokenizer.mask_token_id)

        mask_word = softmax[0, mask_indexes, :]

        if topOne:
            top_word = torch.argmax(mask_word, dim=1)
            res = tokenizer.decode(top_word)
            new_sentence = sentence.replace(tokenizer.mask_token, res)
            masked_result.append(new_sentence)

        else:
            top_words = torch.topk(mask_word, topNum, dim = 1)[1][0]
            loc_list = []
            for top_word in top_words:
                res = tokenizer.decode(top_word)
                res = res.replace(' ', '')
                new_sentence = sentence.replace(tokenizer.mask_token, res)
                loc_list.append(new_sentence)
            masked_result.append(loc_list)

    for sentence in not_masked_sentences:
        not_masked_result.append(sentence)
    
    if not topOne:
        top_results = []

        for i in range(topNum):
            top_results.append([not_masked_result[0], not_masked_result[1], masked_result[0][i], masked_result[1][i]])
        results = [format_result(top_result) for top_result in top_results]
    else:
        results = not_masked_result + masked_result
        results = [format_result(results)]

    show_result(results, cut=True)

def show_result(data, cut=False):
    for i, result in enumerate(data):
        if cut and i == (len(data) - 1):
            result = result.strip('\n')
        print(result)

def format_result(data):

    splitted_sentences = []

    for pair in data:
        #remove spaces
        loc_res = pair.split(' ')
        loc_res = [item for item in loc_res if item.strip() != '']

        #append list of words from sentence
        splitted_sentences.append(loc_res)

    sentences = []

    for splitted_sentence in splitted_sentences:
        loc_res = ' '.join([word for word in splitted_sentence])
        sentences.append(loc_res)
    
    result = ''.join([sentence for sentence in sentences])
        
    return result

# text_eng = "Virgin mind stuck in broken pieces of my dreary life, \nMissed opportunities always throw down the glove. \nHaunting memories are squeezing inside out, \nLet me disconnect from reality!"

text_eng = "In here a tale of a boy who reached the masses \nHis name was Billy and he liked to get high \nA normal guy until the green-skin apocalypse \nA do or die"

print(text_eng)

predict_rhyme(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='eng', fullLines=True)

predict_rhyme(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='eng', fullLines=True, genre='rock')

predict_rhyme(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='eng', fullLines=True, genre='rap')

predict_rhyme(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='eng', fullLines=True, genre='pop')

predict_rhyme(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='eng', fullLines=True, genre='metal')

text_rus = "Как бессонница в час ночной \nМеняет, нелюдимая, облик твой, \nЧьих невольница ты идей? \nЗачем тебе охотиться на людей?"

print(text_rus)

predict_rhyme(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='rus', fullLines=True)

predict_rhyme(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='rus', fullLines=True, genre='rock')

predict_rhyme(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='rus', fullLines=True, genre='rap')

predict_rhyme(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, LANGUAGE='rus', fullLines=True, genre='pop')

from torch.nn import functional as F
import re
import string

def predict_rhyme_second_type(text, modelName='bert', topOne=True, topNum=10, BATCH_SIZE=16, punctuation=True, LANGUAGE='eng', fullLines=False):
    
    tokenizer, model, _ = load_model(modelName=modelName, BATCH_SIZE=BATCH_SIZE, LANGUAGE=LANGUAGE, fullLines=fullLines)

    splitted = text.split('\n')

    masked_sentences, punctuations = [], []

    for i, sentence in enumerate(splitted[:2]):

        loc_res = re.findall(r"[\w']+|[.,!?;]", sentence)
    
        loc_str = ' '.join([word for word in loc_res if word not in string.punctuation])
        loc_str += ' ' + tokenizer.mask_token

        masked_sentences.append(loc_str.strip())

        punctuation = [word for word in loc_res if word in string.punctuation]

        if len(punctuation) != 0: 
            punctuations.append(punctuation[0])
        else:
            punctuations.append('')

    if punctuation:
        masked_sentences = [sentence + punctuations[i] for i, sentence in enumerate(masked_sentences)]

    masked_result = []

    for i, sentence in enumerate(masked_sentences):

        sentence_toks = tokenizer.encode_plus(sentence, return_tensors = "pt")

        outputs = model(**sentence_toks)

        softmax = F.softmax(outputs.logits, dim = - 1)

        mask_indexes = torch.where(sentence_toks["input_ids"][0] == tokenizer.mask_token_id)

        mask_word = softmax[0, mask_indexes, :]

        if topOne:
            top_word = torch.argmax(mask_word, dim=1)
            res = tokenizer.decode(top_word)
            masked_result.append(res)

        else:
            top_words = torch.topk(mask_word, topNum, dim = 1)[1][0]
            loc_list = []
            for top_word in top_words:
                res = tokenizer.decode(top_word)
                loc_list.append(res)
            masked_result.append(loc_list)

    splitted_updated = []

    for i, sentence in enumerate(splitted[2:]):

        loc_res = re.findall(r"[\w']+|[.,!?;]", sentence)

        loc_str = []
        for word in loc_res:
                
            if word not in string.punctuation:
                loc_str.append(word)
        
        loc_res = []
        
        if not topOne:
            for j in range(topNum):
                loc_str_copy = loc_str.copy()
                loc_str_copy[-1] = masked_result[i - 2][j].strip()
                loc_res.append((' '.join([word for word in loc_str_copy])).strip() + punctuations[i - 2])
            splitted_updated.append(loc_res)
        else:
            loc_str_copy = loc_str.copy()
            loc_str_copy[-1] = masked_result[i - 2].strip()
            loc_res.append((' '.join([word for word in loc_str_copy])).strip() + punctuations[i - 2])

            splitted_updated.append(loc_res[0])

    result = []

    if not topOne:
        result = []
        for i in range(topNum): 
            loc_res = splitted[0] + '\n' + splitted[1] + '\n' + splitted_updated[0][i] + '\n' + splitted_updated[1][i]
            result.append('\n')
            result.append(loc_res)
    else:
        loc_res = splitted[0] + '\n' + splitted[1] + '\n' + splitted_updated[0] + '\n' + splitted_updated[1]
        result.append(loc_res)

    show_result(result)

predict_rhyme_second_type(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='eng', fullLines=True)

predict_rhyme_second_type(text_eng, modelName='roberta', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='eng', fullLines=False)

predict_rhyme_second_type(text_eng, modelName='bert', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='eng', fullLines=True)

predict_rhyme_second_type(text_eng, modelName='bert', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='eng', fullLines=False)

predict_rhyme_second_type(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='rus', fullLines=True)

predict_rhyme_second_type(text_rus, modelName='roberta', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='rus', fullLines=False)

predict_rhyme_second_type(text_rus, modelName='bert', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='rus', fullLines=True)

predict_rhyme_second_type(text_rus, modelName='bert', topOne=True, BATCH_SIZE=32, punctuation=True, LANGUAGE='rus', fullLines=False)